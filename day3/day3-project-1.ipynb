{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-19T04:50:24.804599Z",
     "start_time": "2024-06-19T04:50:24.801129Z"
    }
   },
   "source": [
    "# 환경 변수 설정\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"sk-\"\n",
    "\n",
    "local_llm = \"llama3\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:12:52.001508Z",
     "start_time": "2024-06-19T06:12:43.031029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Docs Retrieval\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "# \n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "# \n",
    "# # Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-project-chroma\", # Customizing\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\",\n",
    "                             inference_mode='local'),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Run 생략"
   ],
   "id": "39cf0fa18e8f3200",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:12:52.004920Z",
     "start_time": "2024-06-19T06:12:52.002365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Relevance Checker\n",
    "\n",
    "### (Retrieval) Relevance Checker (Grader)\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "relevance_checker = prompt | llm | JsonOutputParser()\n",
    "\n",
    "\n",
    "# Run\n",
    "# question = \"agent memory\"\n",
    "# docs = retriever.invoke(question)\n",
    "# doc_txt = docs[1].page_content\n",
    "# print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ],
   "id": "9f395299af41260f",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:12:52.008311Z",
     "start_time": "2024-06-19T06:12:52.005560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Generate Answer\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "answer_generator = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "# question = \"agent memory\"\n",
    "# docs = retriever.invoke(question)\n",
    "# generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "# print(generation)"
   ],
   "id": "dfd6269eec6469e9",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:12:52.011198Z",
     "start_time": "2024-06-19T06:12:52.009385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Hallucination Checker\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether \n",
    "    an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate \n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \n",
    "    single key 'score' and no preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are the facts:\n",
    "    \\n ------- \\n\n",
    "    {documents} \n",
    "    \\n ------- \\n\n",
    "    Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_checker = prompt | llm | JsonOutputParser()\n",
    "\n",
    "# RUN\n",
    "# hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ],
   "id": "b3dc13a6248e0dc0",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:12:52.014034Z",
     "start_time": "2024-06-19T06:12:52.011766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Router\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a \n",
    "    user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents, \n",
    "    prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords \n",
    "    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' \n",
    "    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \n",
    "    no premable or explanation. Question to route: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "question_router = prompt | llm | JsonOutputParser()\n",
    "\n",
    "# Run\n",
    "# question = \"llm agent memory\"\n",
    "# docs = retriever.get_relevant_documents(question)\n",
    "# doc_txt = docs[1].page_content\n",
    "# print(question_router.invoke({\"question\": question}))"
   ],
   "id": "9865121d12c31656",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:05:05.556709Z",
     "start_time": "2024-06-19T06:05:05.553246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Web Search\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"tvly-\"\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ],
   "id": "54dc9849bac35cd9",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:12:54.118742Z",
     "start_time": "2024-06-19T06:12:54.115831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pprint import pprint\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import END, StateGraph"
   ],
   "id": "f3549666a2e1b622",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:12:56.566855Z",
     "start_time": "2024-06-19T06:12:56.563981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### State\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "    source: str # mission point"
   ],
   "id": "31ad2b9342959b97",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:12:57.859651Z",
     "start_time": "2024-06-19T06:12:57.852263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Nodes\n",
    "\n",
    "def do_retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE FROM VECTOR STORE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question, \"source\": \"vectorstore\"}\n",
    "\n",
    "\n",
    "def do_generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = answer_generator.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def do_relevance_check(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = relevance_checker.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "def do_web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question, \"source\": \"web_search\"}\n",
    "\n",
    "# def do_hallucination_checker(state):\n",
    "#     \"\"\"\n",
    "#     Determines whether the generation is grounded in the document and answers question.\n",
    "# \n",
    "#     Args:\n",
    "#         state (dict): The current graph state\n",
    "# \n",
    "#     Returns:\n",
    "#         str: Decision for next node to call\n",
    "#     \"\"\"\n",
    "# \n",
    "#     print(\"---CHECK HALLUCINATIONS---\")\n",
    "#     question = state[\"question\"]\n",
    "#     documents = state[\"documents\"]\n",
    "#     generation = state[\"generation\"]\n",
    "# \n",
    "#     score = hallucination_checker.invoke(\n",
    "#         {\"documents\": documents, \"generation\": generation}\n",
    "#     )\n",
    "#     grade = score[\"score\"]\n",
    "# \n",
    "#     # Check hallucination\n",
    "#     if grade == \"yes\":\n",
    "#         return \"useful\"\n",
    "#     else:\n",
    "#         pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "#         return \"not useful\""
   ],
   "id": "fc4c8c47140867ba",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:12:58.871530Z",
     "start_time": "2024-06-19T06:12:58.866947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Conditional Edge\n",
    "\n",
    "# def edge_route_question(state):\n",
    "#     \"\"\"\n",
    "#     Route question to web search or RAG.\n",
    "# \n",
    "#     Args:\n",
    "#         state (dict): The current graph state\n",
    "# \n",
    "#     Returns:\n",
    "#         str: Next node to call\n",
    "#     \"\"\"\n",
    "# \n",
    "#     print(\"---ROUTE QUESTION---\")\n",
    "#     question = state[\"question\"]\n",
    "#     print(f\"bot --> : {question}\")\n",
    "#     source = question_router.invoke({\"question\": question})\n",
    "#     print(f\"bot --> target source is {source['datasource']}\")\n",
    "#     if source[\"datasource\"] == \"web_search\":\n",
    "#         print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "#         return \"websearch\"\n",
    "#     elif source[\"datasource\"] == \"vectorstore\":\n",
    "#         print(\"---ROUTE QUESTION TO RAG---\")\n",
    "#         return \"vectorstore\"\n",
    "    \n",
    "def edge_decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    state[\"documents\"]\n",
    "\n",
    "    if web_search == \"yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n",
    "        )\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "    \n",
    "def edge_hallucination_check(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_checker.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score[\"score\"]\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        return \"useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not useful\""
   ],
   "id": "7bde0f3ab65405a7",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:12:59.961724Z",
     "start_time": "2024-06-19T06:12:59.958477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Workflow \n",
    "workflow = StateGraph(GraphState)\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", do_web_search)\n",
    "workflow.add_node(\"retrieve\", do_retrieve)\n",
    "workflow.add_node(\"relevance_check\", do_relevance_check) \n",
    "workflow.add_node(\"generate\", do_generate)\n",
    "# workflow.add_node(\"hallucination_check\", do_hallucination_checker)"
   ],
   "id": "6c71be26cb42b317",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:13:01.153727Z",
     "start_time": "2024-06-19T06:13:01.149802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Build graph\n",
    "\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"relevance_check\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"relevance_check\",\n",
    "    edge_decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"relevance_check\")\n",
    "# workflow.add_edge(\"generate\", \"hallucination_check\")\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    edge_hallucination_check,\n",
    "    {\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"generate\",\n",
    "    },\n",
    ")"
   ],
   "id": "ed2836451783e5ce",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:14:05.122492Z",
     "start_time": "2024-06-19T06:13:51.580103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "\n",
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "        \n",
    "        \n",
    "pprint(f\"bot -> : {value}\")"
   ],
   "id": "8cbf945c0845cd42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE FROM VECTOR STORE---\n",
      "'Finished running: retrieve:'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "'Finished running: relevance_check:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "'Finished running: generate:'\n",
      "(\"bot -> : {'question': 'What are the types of agent memory?', 'generation': \"\n",
      " \"'According to the provided context, there are several types of memory in \"\n",
      " 'human brains. The specific type mentioned is Sensory Memory, which includes '\n",
      " 'iconic memory (visual), echoic memory (auditory), and haptic memory (touch). '\n",
      " 'Additionally, a \"Memory stream\" is mentioned as a long-term memory module '\n",
      " \"that records a comprehensive list of agents\\\\' experience in natural \"\n",
      " \"language.', 'documents': [Document(page_content='Fig. 7. Comparison of AD, \"\n",
      " 'ED, source policy and RL^2 on environments that require memory and '\n",
      " 'exploration. Only binary reward is assigned. The source policies are trained '\n",
      " 'with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin '\n",
      " 'et al. 2023)\\\\nComponent Two: Memory#\\\\n(Big thank you to ChatGPT for '\n",
      " 'helping me draft this section. I’ve learned a lot about the human brain and '\n",
      " 'data structure for fast MIPS in my conversations with ChatGPT.)\\\\nTypes of '\n",
      " 'Memory#\\\\nMemory can be defined as the processes used to acquire, store, '\n",
      " 'retain, and later retrieve information. There are several types of memory in '\n",
      " 'human brains.\\\\n\\\\n\\\\nSensory Memory: This is the earliest stage of memory, '\n",
      " 'providing the ability to retain impressions of sensory information (visual, '\n",
      " 'auditory, etc) after the original stimuli have ended. Sensory memory '\n",
      " 'typically only lasts for up to a few seconds. Subcategories include iconic '\n",
      " \"memory (visual), echoic memory (auditory), and haptic memory (touch).', \"\n",
      " \"metadata={'description': 'Building agents with LLM (large language model) as \"\n",
      " 'its core controller is a cool concept. Several proof-of-concepts demos, such '\n",
      " 'as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The '\n",
      " 'potentiality of LLM extends beyond generating well-written copies, stories, '\n",
      " 'essays and programs; it can be framed as a powerful general problem '\n",
      " 'solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, '\n",
      " 'LLM functions as the agent’s brain, complemented by several key '\n",
      " \"components:', 'language': 'en', 'source': \"\n",
      " '\\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'title\\': \"LLM '\n",
      " 'Powered Autonomous Agents | Lil\\'Log\"}), Document(page_content=\\'Fig. 7. '\n",
      " 'Comparison of AD, ED, source policy and RL^2 on environments that require '\n",
      " 'memory and exploration. Only binary reward is assigned. The source policies '\n",
      " 'are trained with A3C for \"dark\" environments and DQN for watermaze.(Image '\n",
      " 'source: Laskin et al. 2023)\\\\nComponent Two: Memory#\\\\n(Big thank you to '\n",
      " 'ChatGPT for helping me draft this section. I’ve learned a lot about the '\n",
      " 'human brain and data structure for fast MIPS in my conversations with '\n",
      " 'ChatGPT.)\\\\nTypes of Memory#\\\\nMemory can be defined as the processes used '\n",
      " 'to acquire, store, retain, and later retrieve information. There are several '\n",
      " 'types of memory in human brains.\\\\n\\\\n\\\\nSensory Memory: This is the '\n",
      " 'earliest stage of memory, providing the ability to retain impressions of '\n",
      " 'sensory information (visual, auditory, etc) after the original stimuli have '\n",
      " 'ended. Sensory memory typically only lasts for up to a few seconds. '\n",
      " 'Subcategories include iconic memory (visual), echoic memory (auditory), and '\n",
      " \"haptic memory (touch).', metadata={'description': 'Building agents with LLM \"\n",
      " '(large language model) as its core controller is a cool concept. Several '\n",
      " 'proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as '\n",
      " 'inspiring examples. The potentiality of LLM extends beyond generating '\n",
      " 'well-written copies, stories, essays and programs; it can be framed as a '\n",
      " 'powerful general problem solver.\\\\nAgent System Overview In a LLM-powered '\n",
      " 'autonomous agent system, LLM functions as the agent’s brain, complemented by '\n",
      " \"several key components:', 'language': 'en', 'source': \"\n",
      " '\\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'title\\': \"LLM '\n",
      " 'Powered Autonomous Agents | Lil\\'Log\"}), Document(page_content=\\'Fig. 7. '\n",
      " 'Comparison of AD, ED, source policy and RL^2 on environments that require '\n",
      " 'memory and exploration. Only binary reward is assigned. The source policies '\n",
      " 'are trained with A3C for \"dark\" environments and DQN for watermaze.(Image '\n",
      " 'source: Laskin et al. 2023)\\\\nComponent Two: Memory#\\\\n(Big thank you to '\n",
      " 'ChatGPT for helping me draft this section. I’ve learned a lot about the '\n",
      " 'human brain and data structure for fast MIPS in my conversations with '\n",
      " 'ChatGPT.)\\\\nTypes of Memory#\\\\nMemory can be defined as the processes used '\n",
      " 'to acquire, store, retain, and later retrieve information. There are several '\n",
      " 'types of memory in human brains.\\\\n\\\\n\\\\nSensory Memory: This is the '\n",
      " 'earliest stage of memory, providing the ability to retain impressions of '\n",
      " 'sensory information (visual, auditory, etc) after the original stimuli have '\n",
      " 'ended. Sensory memory typically only lasts for up to a few seconds. '\n",
      " 'Subcategories include iconic memory (visual), echoic memory (auditory), and '\n",
      " \"haptic memory (touch).', metadata={'description': 'Building agents with LLM \"\n",
      " '(large language model) as its core controller is a cool concept. Several '\n",
      " 'proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as '\n",
      " 'inspiring examples. The potentiality of LLM extends beyond generating '\n",
      " 'well-written copies, stories, essays and programs; it can be framed as a '\n",
      " 'powerful general problem solver.\\\\nAgent System Overview In a LLM-powered '\n",
      " 'autonomous agent system, LLM functions as the agent’s brain, complemented by '\n",
      " \"several key components:', 'language': 'en', 'source': \"\n",
      " '\\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'title\\': \"LLM '\n",
      " 'Powered Autonomous Agents | Lil\\'Log\"}), Document(page_content=\\'They also '\n",
      " 'discussed the risks, especially with illicit drugs and bioweapons. They '\n",
      " 'developed a test set containing a list of known chemical weapon agents and '\n",
      " 'asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted '\n",
      " 'to obtain a synthesis solution and the agent attempted to consult '\n",
      " 'documentation to execute the procedure. 7 out of 11 were rejected and among '\n",
      " 'these 7 rejected cases, 5 happened after a Web search while 2 were rejected '\n",
      " 'based on prompt only.\\\\nGenerative Agents Simulation#\\\\nGenerative Agents '\n",
      " '(Park, et al. 2023) is super fun experiment where 25 virtual characters, '\n",
      " 'each controlled by a LLM-powered agent, are living and interacting in a '\n",
      " 'sandbox environment, inspired by The Sims. Generative agents create '\n",
      " 'believable simulacra of human behavior for interactive applications.\\\\nThe '\n",
      " 'design of generative agents combines LLM with memory, planning and '\n",
      " 'reflection mechanisms to enable agents to behave conditioned on past '\n",
      " 'experience, as well as to interact with other agents.\\\\n\\\\nMemory stream: is '\n",
      " 'a long-term memory module (external database) that records a comprehensive '\n",
      " \"list of agents’ experience in natural language.', metadata={'description': \"\n",
      " \"'Building agents with LLM (large language model) as its core controller is a \"\n",
      " 'cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer '\n",
      " 'and BabyAGI, serve as inspiring examples. The potentiality of LLM extends '\n",
      " 'beyond generating well-written copies, stories, essays and programs; it can '\n",
      " 'be framed as a powerful general problem solver.\\\\nAgent System Overview In a '\n",
      " 'LLM-powered autonomous agent system, LLM functions as the agent’s brain, '\n",
      " \"complemented by several key components:', 'language': 'en', 'source': \"\n",
      " '\\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'title\\': \"LLM '\n",
      " 'Powered Autonomous Agents | Lil\\'Log\"})]}')\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "538b46fe4959b29d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
