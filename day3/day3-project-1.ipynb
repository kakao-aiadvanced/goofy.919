{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-19T04:50:24.804599Z",
     "start_time": "2024-06-19T04:50:24.801129Z"
    }
   },
   "source": [
    "# 환경 변수 설정\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"sk-\"\n",
    "\n",
    "local_llm = \"llama3\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:04:50.725214Z",
     "start_time": "2024-06-19T06:04:41.694584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Docs Retrieval\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "# \n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "# \n",
    "# # Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-project-chroma\", # Customizing\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\",\n",
    "                             inference_mode='local'),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Run 생략"
   ],
   "id": "39cf0fa18e8f3200",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:04:58.086128Z",
     "start_time": "2024-06-19T06:04:58.082911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Relevance Checker\n",
    "\n",
    "### (Retrieval) Relevance Checker (Grader)\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "relevance_checker = prompt | llm | JsonOutputParser()\n",
    "\n",
    "\n",
    "# Run\n",
    "# question = \"agent memory\"\n",
    "# docs = retriever.invoke(question)\n",
    "# doc_txt = docs[1].page_content\n",
    "# print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ],
   "id": "9f395299af41260f",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:05:00.944788Z",
     "start_time": "2024-06-19T06:05:00.940199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Generate Answer\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "answer_generator = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "# question = \"agent memory\"\n",
    "# docs = retriever.invoke(question)\n",
    "# generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "# print(generation)"
   ],
   "id": "dfd6269eec6469e9",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T04:58:53.993515Z",
     "start_time": "2024-06-19T04:58:53.990006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Hallucination Checker\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether \n",
    "    an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate \n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \n",
    "    single key 'score' and no preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are the facts:\n",
    "    \\n ------- \\n\n",
    "    {documents} \n",
    "    \\n ------- \\n\n",
    "    Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_checker = prompt | llm | JsonOutputParser()\n",
    "\n",
    "# RUN\n",
    "# hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ],
   "id": "b3dc13a6248e0dc0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:05:03.923896Z",
     "start_time": "2024-06-19T06:05:03.920979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Router\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a \n",
    "    user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents, \n",
    "    prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords \n",
    "    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' \n",
    "    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \n",
    "    no premable or explanation. Question to route: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "question_router = prompt | llm | JsonOutputParser()\n",
    "\n",
    "# Run\n",
    "# question = \"llm agent memory\"\n",
    "# docs = retriever.get_relevant_documents(question)\n",
    "# doc_txt = docs[1].page_content\n",
    "# print(question_router.invoke({\"question\": question}))"
   ],
   "id": "9865121d12c31656",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:05:05.556709Z",
     "start_time": "2024-06-19T06:05:05.553246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Web Search\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"tvly-\"\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ],
   "id": "54dc9849bac35cd9",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:05:08.036251Z",
     "start_time": "2024-06-19T06:05:08.033460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pprint import pprint\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import END, StateGraph"
   ],
   "id": "f3549666a2e1b622",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:05:10.224118Z",
     "start_time": "2024-06-19T06:05:10.221249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### State\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]"
   ],
   "id": "31ad2b9342959b97",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:05:19.161949Z",
     "start_time": "2024-06-19T06:05:19.155236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Nodes\n",
    "\n",
    "def do_retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE FROM VECTOR STORE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def do_generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = answer_generator.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def do_relevance_check(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = relevance_checker.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "def do_web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "# def do_hallucination_checker(state):\n",
    "#     \"\"\"\n",
    "#     Determines whether the generation is grounded in the document and answers question.\n",
    "# \n",
    "#     Args:\n",
    "#         state (dict): The current graph state\n",
    "# \n",
    "#     Returns:\n",
    "#         str: Decision for next node to call\n",
    "#     \"\"\"\n",
    "# \n",
    "#     print(\"---CHECK HALLUCINATIONS---\")\n",
    "#     question = state[\"question\"]\n",
    "#     documents = state[\"documents\"]\n",
    "#     generation = state[\"generation\"]\n",
    "# \n",
    "#     score = hallucination_checker.invoke(\n",
    "#         {\"documents\": documents, \"generation\": generation}\n",
    "#     )\n",
    "#     grade = score[\"score\"]\n",
    "# \n",
    "#     # Check hallucination\n",
    "#     if grade == \"yes\":\n",
    "#         return \"useful\"\n",
    "#     else:\n",
    "#         pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "#         return \"not useful\""
   ],
   "id": "fc4c8c47140867ba",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:05:20.830488Z",
     "start_time": "2024-06-19T06:05:20.825571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Conditional Edge\n",
    "\n",
    "# def edge_route_question(state):\n",
    "#     \"\"\"\n",
    "#     Route question to web search or RAG.\n",
    "# \n",
    "#     Args:\n",
    "#         state (dict): The current graph state\n",
    "# \n",
    "#     Returns:\n",
    "#         str: Next node to call\n",
    "#     \"\"\"\n",
    "# \n",
    "#     print(\"---ROUTE QUESTION---\")\n",
    "#     question = state[\"question\"]\n",
    "#     print(f\"bot --> : {question}\")\n",
    "#     source = question_router.invoke({\"question\": question})\n",
    "#     print(f\"bot --> target source is {source['datasource']}\")\n",
    "#     if source[\"datasource\"] == \"web_search\":\n",
    "#         print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "#         return \"websearch\"\n",
    "#     elif source[\"datasource\"] == \"vectorstore\":\n",
    "#         print(\"---ROUTE QUESTION TO RAG---\")\n",
    "#         return \"vectorstore\"\n",
    "    \n",
    "def edge_decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    state[\"documents\"]\n",
    "\n",
    "    if web_search == \"yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n",
    "        )\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "    \n",
    "def edge_hallucination_check(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_checker.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score[\"score\"]\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        return \"useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not useful\""
   ],
   "id": "7bde0f3ab65405a7",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:05:22.374713Z",
     "start_time": "2024-06-19T06:05:22.371506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Workflow \n",
    "workflow = StateGraph(GraphState)\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", do_web_search)\n",
    "workflow.add_node(\"retrieve\", do_retrieve)\n",
    "workflow.add_node(\"relevance_check\", do_relevance_check) \n",
    "workflow.add_node(\"generate\", do_generate)\n",
    "# workflow.add_node(\"hallucination_check\", do_hallucination_checker)"
   ],
   "id": "6c71be26cb42b317",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:05:41.051126Z",
     "start_time": "2024-06-19T06:05:41.023463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Build graph\n",
    "\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"relevance_check\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"relevance_check\",\n",
    "    edge_decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"relevance_check\")\n",
    "# workflow.add_edge(\"generate\", \"hallucination_check\")\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    edge_hallucination_check,\n",
    "    {\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"generate\",\n",
    "    },\n",
    ")"
   ],
   "id": "ed2836451783e5ce",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Already found path for node '__start__'.\nFor multiple edges, use StateGraph with an annotated state key.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[47], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m### Build graph\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mworkflow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_entry_point\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mretrieve\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m workflow\u001B[38;5;241m.\u001B[39madd_edge(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mretrieve\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelevance_check\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      7\u001B[0m workflow\u001B[38;5;241m.\u001B[39madd_conditional_edges(\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelevance_check\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      9\u001B[0m     edge_decide_to_generate,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m     },\n\u001B[1;32m     14\u001B[0m )\n",
      "File \u001B[0;32m~/Personal/langchain_study/.venv/lib/python3.11/site-packages/langgraph/graph/graph.py:233\u001B[0m, in \u001B[0;36mGraph.set_entry_point\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    224\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mset_entry_point\u001B[39m(\u001B[38;5;28mself\u001B[39m, key: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    225\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Specifies the first node to be called in the graph.\u001B[39;00m\n\u001B[1;32m    226\u001B[0m \n\u001B[1;32m    227\u001B[0m \u001B[38;5;124;03m    Parameters:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;124;03m        None\u001B[39;00m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 233\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_edge\u001B[49m\u001B[43m(\u001B[49m\u001B[43mSTART\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Personal/langchain_study/.venv/lib/python3.11/site-packages/langgraph/graph/state.py:186\u001B[0m, in \u001B[0;36mStateGraph.add_edge\u001B[0;34m(self, start_key, end_key)\u001B[0m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Adds a directed edge from the start node to the end node.\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \n\u001B[1;32m    173\u001B[0m \u001B[38;5;124;03mIf the graph transitions to the start_key node, it will always transition to the end_key node next.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    183\u001B[0m \u001B[38;5;124;03m    None\u001B[39;00m\n\u001B[1;32m    184\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(start_key, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 186\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_edge\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstart_key\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompiled:\n\u001B[1;32m    189\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[1;32m    190\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAdding an edge to a graph that has already been compiled. This will \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    191\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnot be reflected in the compiled graph.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    192\u001B[0m     )\n",
      "File \u001B[0;32m~/Personal/langchain_study/.venv/lib/python3.11/site-packages/langgraph/graph/graph.py:166\u001B[0m, in \u001B[0;36mGraph.add_edge\u001B[0;34m(self, start_key, end_key)\u001B[0m\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSTART cannot be an end node\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msupport_multiple_edges \u001B[38;5;129;01mand\u001B[39;00m start_key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mset\u001B[39m(\n\u001B[1;32m    164\u001B[0m     start \u001B[38;5;28;01mfor\u001B[39;00m start, _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39medges\n\u001B[1;32m    165\u001B[0m ):\n\u001B[0;32m--> 166\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    167\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAlready found path for node \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstart_key\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    168\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFor multiple edges, use StateGraph with an annotated state key.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    169\u001B[0m     )\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39medges\u001B[38;5;241m.\u001B[39madd((start_key, end_key))\n",
      "\u001B[0;31mValueError\u001B[0m: Already found path for node '__start__'.\nFor multiple edges, use StateGraph with an annotated state key."
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:06:03.164413Z",
     "start_time": "2024-06-19T06:05:49.841332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "\n",
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "        \n",
    "        \n",
    "pprint(value[\"generation\"])"
   ],
   "id": "8cbf945c0845cd42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE FROM VECTOR STORE---\n",
      "'Finished running: retrieve:'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "'Finished running: relevance_check:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "'Finished running: generate:'\n",
      "('According to the provided context, there are several types of memory '\n",
      " 'mentioned. The main type of memory discussed is Sensory Memory, which '\n",
      " 'includes iconic memory (visual), echoic memory (auditory), and haptic memory '\n",
      " '(touch). Additionally, a \"Memory stream\" is mentioned as a long-term memory '\n",
      " \"module that records agents' experience in natural language.\")\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "538b46fe4959b29d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
